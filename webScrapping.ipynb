{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "query_words = {'ciberseguridad', 'seguridad informatica', 'filtran datos', 'sitio web', 'cibercrimen'}\n",
    "\n",
    "key_words = {}\n",
    "\n",
    "\n",
    "def make_url(page, search_string):\n",
    "    \"\"\" Creates a url, querying prensalibre for the given search_string on a given page.\n",
    "    Pages start on 1, ends on code 404\n",
    "    parameters:\n",
    "        page(int): number of page\n",
    "        search_string(str): query string to be used\n",
    "    returns:\n",
    "        str: resulting url\n",
    "    \"\"\"\n",
    "\n",
    "    query_string = str(search_string).replace(\" \", \"+\") \n",
    "    return f\"https://www.prensalibre.com/page/{str(page)}/?s={query_string}\"\n",
    "\n",
    "# https://www.prensalibre.com/?s=\n",
    "\n",
    "def crawler(query_words:set, key_words:set, limit = 1): \n",
    "    \"\"\"\n",
    "    main(): iterar sobre la pagina principal.\n",
    "            recibir el resultado de make_url.\n",
    "    parameters: \n",
    "        limit(int): limit of pages per query\n",
    "        query_words(set): words to be queried on the web page\n",
    "        key_words(set): key words to be searched inside the page results\n",
    "    returns:\n",
    "        pd.DataFrame: important data \n",
    "        \n",
    "    \"\"\"\n",
    "    queries_urls = []\n",
    "    \n",
    "    for query_word in query_words:\n",
    "        page_num = 0 \n",
    "        while(True):\n",
    "            page_num+=1\n",
    "            if limit < page_num:\n",
    "                break\n",
    "            raw=requests.get(make_url(page_num, query_word))\n",
    "            html=raw.text\n",
    "            response_status = raw.status_code\n",
    "            if response_status == 404 or response_status == '404':\n",
    "                break\n",
    "            \n",
    "            bs4= BeautifulSoup(html,\"html.parser\")\n",
    "            results = get_results(bs4)\n",
    "            queries_urls.extend(results)\n",
    "    print(queries_urls)\n",
    "    data = []\n",
    "    for url in queries_urls:\n",
    "        raw=requests.get(url)\n",
    "        html=raw.text\n",
    "        response_status = raw.status_code\n",
    "        if response_status == 404 or response_status == '404':\n",
    "            next\n",
    "        bs4= BeautifulSoup(html,\"html.parser\")\n",
    "        page_data = analyse_page(bs4)\n",
    "        page_data['html'] = html # this is to add html to the pandas dataframe\n",
    "        data.append(page_data)\n",
    "    df = pd.DataFrame(data)\n",
    "    # print(df.head(2))\n",
    "        \n",
    "        \n",
    "    return df\n",
    "            #analyse_page(bs4, key_words)\n",
    "            \n",
    "            \n",
    "\n",
    "def get_results(soups):\n",
    "    \"\"\" gets the results list from a html\n",
    "    parameters:\n",
    "        bs4(BeautifulSoup): html of the prensalibre query webpage\n",
    "    returns:\n",
    "        list: list containing the url\n",
    "    \"\"\"\n",
    "    result_list = []\n",
    "    bs4= soups\n",
    "    \n",
    "    h1= str(bs4.select(\"h1\", { 'class': 'story-title'}))\n",
    "    h1 = BeautifulSoup(h1)\n",
    "    a_type= h1.find_all(\"a\")\n",
    "    for link in a_type:\n",
    "        result_list.append(link.get('href'))\n",
    "\n",
    "    return(result_list)\n",
    "\n",
    "def analyse_page(html_soup):\n",
    "    \"\"\"\n",
    "    Searches for instances of keywords\n",
    "    parameters:\n",
    "        bs4(BeautifulSoup): html of the prensalibre single new webpage\n",
    "    returns:\n",
    "        dict: publishing date, author, and tags archive listings.\n",
    "    \"\"\"\n",
    "    # fecha de publicaciÃ³n\n",
    "    publishing_date = html_soup.find(\"time\")\n",
    "    if publishing_date: publishing_date = publishing_date.get(\"datetime\")\n",
    "    author = html_soup.find(\"span\", {'class':'author vcard'})\n",
    "    if author: author = author.text\n",
    "    tags = html_soup.find(\"div\", {'class': 'tag-list-podcast'})\n",
    "    if tags: tags_archive = [t.text.upper() for t in tags.findAll('a')]\n",
    "    else: tags_archive = []\n",
    "    for element in {'plus-title', 'sart-title'}:\n",
    "        title = html_soup.find(\"h1\", {\"class\":element})\n",
    "        if title: break\n",
    "    return {\"publishing_date\":publishing_date,\"author\":author,\"tags_archive\":tags_archive, \"title\":title}\n",
    "    \n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    df = crawler(query_words=query_words, key_words=key_words, limit = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('info.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "d94c1f73a58e90ee80d460cbc9ce8fbd1c7ad7f0ec4d5384ae0f55890aa19e84"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
